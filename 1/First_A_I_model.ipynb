{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4stimliD+8weUGkCbXCEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkrodev/AI_Tutorial/blob/main/1/First_A_I_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LV1nMHaB_s9e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 3*x-1\n",
        "\n",
        "x=np.arange(-2,2.1,0.1)\n",
        "print(x)\n",
        "y=np.array([f(_) for _ in x])\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvcfaK6BomtS",
        "outputId": "5434d3fd-4fca-4322-d22e-d50588eeeeba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.00000000e+00 -1.90000000e+00 -1.80000000e+00 -1.70000000e+00\n",
            " -1.60000000e+00 -1.50000000e+00 -1.40000000e+00 -1.30000000e+00\n",
            " -1.20000000e+00 -1.10000000e+00 -1.00000000e+00 -9.00000000e-01\n",
            " -8.00000000e-01 -7.00000000e-01 -6.00000000e-01 -5.00000000e-01\n",
            " -4.00000000e-01 -3.00000000e-01 -2.00000000e-01 -1.00000000e-01\n",
            "  1.77635684e-15  1.00000000e-01  2.00000000e-01  3.00000000e-01\n",
            "  4.00000000e-01  5.00000000e-01  6.00000000e-01  7.00000000e-01\n",
            "  8.00000000e-01  9.00000000e-01  1.00000000e+00  1.10000000e+00\n",
            "  1.20000000e+00  1.30000000e+00  1.40000000e+00  1.50000000e+00\n",
            "  1.60000000e+00  1.70000000e+00  1.80000000e+00  1.90000000e+00\n",
            "  2.00000000e+00]\n",
            "[-7.  -6.7 -6.4 -6.1 -5.8 -5.5 -5.2 -4.9 -4.6 -4.3 -4.  -3.7 -3.4 -3.1\n",
            " -2.8 -2.5 -2.2 -1.9 -1.6 -1.3 -1.  -0.7 -0.4 -0.1  0.2  0.5  0.8  1.1\n",
            "  1.4  1.7  2.   2.3  2.6  2.9  3.2  3.5  3.8  4.1  4.4  4.7  5. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "#model = tf.keras.models.Sequential([...]): Ten fragment kodu tworzy nowy model sekwencyjny za pomocą klasy Sequential z biblioteki TensorFlow.\n",
        "#Model sekwencyjny to po prostu sekwencja warstw, w której dane przekazywane są z jednej warstwy do drugiej w kolejności, w jakiej zostały zdefiniowane.\n",
        "\n",
        "\n",
        "  tf.keras.layers.Dense(1, input_shape=(1,))\n",
        "\n",
        "#tf.keras.layers.Dense(1, input_shape=(1,)): Jest to warstwa gęsta (Dense), czyli każdy neuron w tej warstwie jest połączony z każdym neuronem z poprzedniej warstwy.\n",
        "#Warstwa ta posiada jeden neuron, co jest określone jako argument funkcji - 1.\n",
        "#Argument input_shape=(1,) oznacza, że nasze dane wejściowe mają jeden wymiar. W naszym przypadku, mamy tylko jedną zmienną wejściową (czyli x a dokładniej całą tablicę x-ów od -2 do 2), więc jest to 1.\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#model.summary(): Ta linia kodu służy do wygenerowania podsumowania architektury modelu.\n",
        "#Wypisuje ona informacje o każdej warstwie modelu, włącznie z typem warstwy, liczbą parametrów i kształtem danych na każdej warstwie.\n",
        "\n",
        "\n",
        "\n",
        "#Podsumowując, ten fragment kodu tworzy prosty model sekwencyjny, który zawiera jedną warstwę gęstą z jednym neuronem,\n",
        "#gdzie dane wejściowe mają jeden wymiar. Funkcja summary() służy do wyświetlenia szczegółów tej architektury modelu."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW-wUmpapy-Y",
        "outputId": "2cad5e61-e68d-478b-b993-bd07e2d67c77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2 (8.00 Byte)\n",
            "Trainable params: 2 (8.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Model: \"sequential_1\": Jest to nazwa modelu, który został utworzony. W tym przypadku model nazwany jest \"sequential_1\", co oznacza, że jest to pierwszy model sekwencyjny w naszym kodzie.\n",
        "    Numer \"1\" dodany do nazwy wynika z faktu, że jest to pierwszy model sekwencyjny utworzony w naszym skrypcie.\n",
        "\n",
        "    Layer (type): Tutaj prezentowane są warstwy, które składają się na nasz model. W naszym przypadku mamy jedną warstwę.\n",
        "\n",
        "    Output Shape: Jest to kształt danych wyjściowych z każdej warstwy. W warstwie dense_1 mamy kształt wyjściowy (None, 1). Oznacza to, że dla dowolnej liczby przykładów (None) na wejściu, warstwa ta generuje pojedynczą wartość wyjściową.\n",
        "\n",
        "    Param #: To liczba parametrów w każdej warstwie. W przypadku warstwy dense_1 mamy 2 parametry. Pierwsza liczba (2) to wagi połączeń między neuronami, a druga liczba (1) to przesunięcie (bias). Razem dają one łącznie 2 parametry.\n",
        "\n",
        "    Total params: To suma wszystkich parametrów w modelu. W naszym przypadku jest to 2 parametry.\n",
        "\n",
        "    Trainable params: Liczba parametrów, które będą trenowane podczas procesu uczenia modelu. W naszym przypadku wszystkie 2 parametry są trenowalne.\n",
        "\n",
        "    Non-trainable params: Liczba parametrów, które nie będą trenowane podczas procesu uczenia modelu. W naszym przypadku nie mamy takich parametrów, dlatego ta wartość wynosi 0."
      ],
      "metadata": {
        "id": "C9TijKhmDq-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
        "\n",
        "#model.compile: Ta metoda służy do skompilowania modelu po jego zdefiniowaniu i zanim zostanie on wytrenowany.\n",
        "#W tym etapie model jest gotowy do uczenia, ale musi być skonfigurowany, aby wiedział, jakie metryki używać podczas uczenia oraz jakie optymalizatory i funkcje straty wykorzystać.\n",
        "\n",
        "#loss='mse': Parametr loss (strata) określa funkcję straty, która będzie minimalizowana podczas procesu uczenia.\n",
        "#W tym przypadku 'mse' oznacza błąd średniokwadratowy (mean squared error), który jest często używany w przypadku regresji, gdzie przewidywane wartości są liczbowe.\n",
        "\n",
        "#optimizer=tf.keras.optimizers.Adam(learning_rate=0.1): Parametr optimizer (optymalizator) określa metodę aktualizacji wag modelu w celu minimalizacji funkcji straty podczas uczenia się.\n",
        "#Tutaj używamy optymalizatora Adam, który jest popularnym algorytmem optymalizacyjnym w uczeniu maszynowym.\n",
        "#learning_rate=0.1 określa szybkość uczenia się modelu, czyli jak dużo model ma \"uczyć się\" podczas każdej iteracji.\n",
        "#Im wyższy współczynnik uczenia się, tym większe są zmiany wag modelu w kierunku minimalizacji straty.\n",
        "\n",
        "#Warto zaznaczyć, że wybór optymalnej wartości learning_rate to często proces eksperymentalny, który wymaga próbowania różnych wartości i oceny ich wpływu na skuteczność uczenia modelu.\n",
        "#W przypadku dużych wartości learning_rate istnieje ryzyko, że proces uczenia się będzie niestabilny lub będzie trudniejszy do optymalizacji.\n",
        "#Dlatego zaleca się rozpoczęcie od mniejszych wartości i stopniowe zwiększanie, jeśli to konieczne."
      ],
      "metadata": {
        "id": "jyyCq_GbsOFO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x,y, epochs=100)\n",
        "\n",
        "#model.fit: Jest to metoda obiektu model, która rozpoczyna proces trenowania naszego modelu maszynowego. W tym przypadku nasz model jest instancją klasy Sequential z biblioteki TensorFlow.\n",
        "#x, y: Te argumenty to nasze dane treningowe. W przypadku naszego kodu, x zawiera nasze dane wejściowe, a y to odpowiadające im etykiety lub wartości docelowe, które chcemy, aby nasz model przewidywał.\n",
        "#!!! WAŻNE !!! Czyli x to tablica od -2 do 2 a y to tablica wywołania funkcji na zbiorze x !!! WAŻNE !!!\n",
        "#epochs=100: Jest to parametr, który określa, ile razy model przejdzie przez cały zbiór danych treningowych podczas procesu uczenia się. Każde przejście przez zbiór danych treningowych to jedna epoka.\n",
        "#W naszym przypadku, ustawiliśmy epochs na 100, co oznacza, że nasz model będzie trenowany na danych treningowych przez 100 epok."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXp2wAB1saxc",
        "outputId": "b95e362c-a138-4c3c-95c8-5bc31a667132"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 1s 15ms/step - loss: 14.1527\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 12.0794\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 10.4391\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 8.8980\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7.5193\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.2723\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.2845\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.4240\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.6157\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.9759\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.3605\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.8746\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4557\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1210\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8168\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5620\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3665\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2388\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1224\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0600\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0244\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0097\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0078\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0119\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0185\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0251\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0303\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0332\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0342\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0337\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0316\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0299\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0273\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0245\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0217\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0184\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0149\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0112\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0081\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0055\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0035\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0020\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9.6360e-04\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 4.3870e-04\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5765e-04\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 8.5740e-05\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.0920e-04\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.8639e-04\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.6273e-04\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.2341e-04\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.6537e-04\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.7717e-04\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.7949e-04\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.5819e-04\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 3.2124e-04\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.7640e-04\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1642e-04\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5943e-04\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0683e-04\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.4795e-05\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.4933e-05\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.6944e-05\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 8.0623e-06\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.1590e-06\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2229e-06\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.0977e-06\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 5.6092e-06\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 7.0800e-06\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 8.2443e-06\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 9.2855e-06\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 9.3758e-06\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 8.7885e-06\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 7.5051e-06\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 6.0123e-06\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.3866e-06\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 2.9312e-06\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.8182e-06\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0407e-06\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 5.4913e-07\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.6014e-07\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1.3745e-07\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 9.9216e-08\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2465e-07\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.7722e-07\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.4586e-07\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.9812e-07\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2179e-07\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.1751e-07\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.8420e-07\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.2465e-07\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.6801e-07\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.0921e-07\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 6.3018e-08\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.2865e-08\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4096e-08\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.8965e-09\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.4413e-09\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.2968e-09\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6.1259e-09\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 8.7818e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x784f0e81cdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds=model.predict(x)\n",
        "\n",
        "\n",
        "#preds=model.predict(x): Ta linijka kodu służy do przewidywania wartości na podstawie danych wejściowych x za pomocą wytrenowanego modelu.\n",
        "#Metoda predict() jest wywoływana na naszym wytrenowanym modelu, a jako argument podajemy dane wejściowe x.\n",
        "#Model wykorzystuje wagi, które zostały nauczone podczas procesu trenowania, aby wygenerować prognozy dla danych wejściowych x.\n",
        "\n",
        "\n",
        "print(y) #Wyniki konkretne jakie powinny być\n",
        "print(preds.flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6jLUHUjs6Wz",
        "outputId": "7b043716-f180-4ddc-ba2d-e1337445f783"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 10ms/step\n",
            "[-7.  -6.7 -6.4 -6.1 -5.8 -5.5 -5.2 -4.9 -4.6 -4.3 -4.  -3.7 -3.4 -3.1\n",
            " -2.8 -2.5 -2.2 -1.9 -1.6 -1.3 -1.  -0.7 -0.4 -0.1  0.2  0.5  0.8  1.1\n",
            "  1.4  1.7  2.   2.3  2.6  2.9  3.2  3.5  3.8  4.1  4.4  4.7  5. ]\n",
            "[-7.0001674  -6.7001586  -6.4001493  -6.100141   -5.8001323  -5.5001235\n",
            " -5.2001143  -4.9001055  -4.600097   -4.3000884  -4.000079   -3.7000704\n",
            " -3.4000618  -3.1000528  -2.800044   -2.5000353  -2.2000265  -1.9000176\n",
            " -1.6000087  -1.3        -0.9999911  -0.6999823  -0.39997345 -0.09996462\n",
            "  0.20004421  0.500053    0.8000619   1.1000705   1.4000795   1.700088\n",
            "  2.000097    2.300106    2.6001148   2.9001234   3.2001321   3.5001414\n",
            "  3.8001502   4.100159    4.4001675   4.7001767   5.0001855 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test=np.arange(-100,101,1)\n",
        "print(x_test)\n",
        "y_test=np.array([f(_) for _ in x_test])\n",
        "print(y_test)\n",
        "#Tworzenie danych testowych"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOtzY4G-tJL7",
        "outputId": "3d651134-4b50-43fd-9580-c633dbe661cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-100  -99  -98  -97  -96  -95  -94  -93  -92  -91  -90  -89  -88  -87\n",
            "  -86  -85  -84  -83  -82  -81  -80  -79  -78  -77  -76  -75  -74  -73\n",
            "  -72  -71  -70  -69  -68  -67  -66  -65  -64  -63  -62  -61  -60  -59\n",
            "  -58  -57  -56  -55  -54  -53  -52  -51  -50  -49  -48  -47  -46  -45\n",
            "  -44  -43  -42  -41  -40  -39  -38  -37  -36  -35  -34  -33  -32  -31\n",
            "  -30  -29  -28  -27  -26  -25  -24  -23  -22  -21  -20  -19  -18  -17\n",
            "  -16  -15  -14  -13  -12  -11  -10   -9   -8   -7   -6   -5   -4   -3\n",
            "   -2   -1    0    1    2    3    4    5    6    7    8    9   10   11\n",
            "   12   13   14   15   16   17   18   19   20   21   22   23   24   25\n",
            "   26   27   28   29   30   31   32   33   34   35   36   37   38   39\n",
            "   40   41   42   43   44   45   46   47   48   49   50   51   52   53\n",
            "   54   55   56   57   58   59   60   61   62   63   64   65   66   67\n",
            "   68   69   70   71   72   73   74   75   76   77   78   79   80   81\n",
            "   82   83   84   85   86   87   88   89   90   91   92   93   94   95\n",
            "   96   97   98   99  100]\n",
            "[-301 -298 -295 -292 -289 -286 -283 -280 -277 -274 -271 -268 -265 -262\n",
            " -259 -256 -253 -250 -247 -244 -241 -238 -235 -232 -229 -226 -223 -220\n",
            " -217 -214 -211 -208 -205 -202 -199 -196 -193 -190 -187 -184 -181 -178\n",
            " -175 -172 -169 -166 -163 -160 -157 -154 -151 -148 -145 -142 -139 -136\n",
            " -133 -130 -127 -124 -121 -118 -115 -112 -109 -106 -103 -100  -97  -94\n",
            "  -91  -88  -85  -82  -79  -76  -73  -70  -67  -64  -61  -58  -55  -52\n",
            "  -49  -46  -43  -40  -37  -34  -31  -28  -25  -22  -19  -16  -13  -10\n",
            "   -7   -4   -1    2    5    8   11   14   17   20   23   26   29   32\n",
            "   35   38   41   44   47   50   53   56   59   62   65   68   71   74\n",
            "   77   80   83   86   89   92   95   98  101  104  107  110  113  116\n",
            "  119  122  125  128  131  134  137  140  143  146  149  152  155  158\n",
            "  161  164  167  170  173  176  179  182  185  188  191  194  197  200\n",
            "  203  206  209  212  215  218  221  224  227  230  233  236  239  242\n",
            "  245  248  251  254  257  260  263  266  269  272  275  278  281  284\n",
            "  287  290  293  296  299]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = model.predict(x_test)\n",
        "print(y_test)\n",
        "print(pred_test.flatten())\n",
        "\n",
        "#pred_test = model.predict(x_test): Ta linijka kodu używa wytrenowanego modelu, aby przewidzieć wartości na podstawie danych testowych x_test.\n",
        "#Metoda predict() jest wywoływana na modelu, a jako argument podajemy dane testowe x_test.\n",
        "#Model wykorzystuje nauczone wagi, aby wygenerować prognozy dla danych testowych x_test."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGlqxNnBtvW5",
        "outputId": "78cf71a2-c2cd-48c0-fb4d-62a52259b826"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 10ms/step\n",
            "[-301 -298 -295 -292 -289 -286 -283 -280 -277 -274 -271 -268 -265 -262\n",
            " -259 -256 -253 -250 -247 -244 -241 -238 -235 -232 -229 -226 -223 -220\n",
            " -217 -214 -211 -208 -205 -202 -199 -196 -193 -190 -187 -184 -181 -178\n",
            " -175 -172 -169 -166 -163 -160 -157 -154 -151 -148 -145 -142 -139 -136\n",
            " -133 -130 -127 -124 -121 -118 -115 -112 -109 -106 -103 -100  -97  -94\n",
            "  -91  -88  -85  -82  -79  -76  -73  -70  -67  -64  -61  -58  -55  -52\n",
            "  -49  -46  -43  -40  -37  -34  -31  -28  -25  -22  -19  -16  -13  -10\n",
            "   -7   -4   -1    2    5    8   11   14   17   20   23   26   29   32\n",
            "   35   38   41   44   47   50   53   56   59   62   65   68   71   74\n",
            "   77   80   83   86   89   92   95   98  101  104  107  110  113  116\n",
            "  119  122  125  128  131  134  137  140  143  146  149  152  155  158\n",
            "  161  164  167  170  173  176  179  182  185  188  191  194  197  200\n",
            "  203  206  209  212  215  218  221  224  227  230  233  236  239  242\n",
            "  245  248  251  254  257  260  263  266  269  272  275  278  281  284\n",
            "  287  290  293  296  299]\n",
            "[-301.00882   -298.00873   -295.00864   -292.00854   -289.00848\n",
            " -286.0084    -283.0083    -280.0082    -277.00812   -274.00803\n",
            " -271.00793   -268.00784   -265.00775   -262.00766   -259.0076\n",
            " -256.00748   -253.0074    -250.00731   -247.00722   -244.00713\n",
            " -241.00703   -238.00696   -235.00687   -232.00677   -229.00668\n",
            " -226.0066    -223.00652   -220.00642   -217.00633   -214.00624\n",
            " -211.00616   -208.00607   -205.00598   -202.00589   -199.00581\n",
            " -196.00572   -193.00563   -190.00554   -187.00545   -184.00537\n",
            " -181.00528   -178.00519   -175.0051    -172.00502   -169.00493\n",
            " -166.00484   -163.00475   -160.00465   -157.00458   -154.00449\n",
            " -151.0044    -148.0043    -145.00423   -142.00414   -139.00404\n",
            " -136.00395   -133.00386   -130.00378   -127.0037    -124.00361\n",
            " -121.00352   -118.00343   -115.00334   -112.00326   -109.00317\n",
            " -106.00308   -103.00299   -100.00291    -97.002815   -94.00272\n",
            "  -91.00264    -88.00255    -85.002464   -82.00237    -79.00229\n",
            "  -76.0022     -73.00211    -70.00202    -67.00193    -64.00185\n",
            "  -61.001755   -58.001667   -55.00158    -52.00149    -49.001404\n",
            "  -46.001316   -43.00123    -40.00114    -37.001053   -34.00096\n",
            "  -31.000872   -28.000784   -25.000696   -22.000608   -19.00052\n",
            "  -16.000431   -13.000344   -10.000257    -7.0001674   -4.000079\n",
            "   -0.9999911    2.000097     5.0001855    8.000274    11.000361\n",
            "   14.000449    17.00054     20.000628    23.000715    26.000803\n",
            "   29.00089     32.000977    35.00107     38.001156    41.001244\n",
            "   44.00133     47.00142     50.001507    53.001595    56.001682\n",
            "   59.00177     62.00186     65.001945    68.00204     71.00213\n",
            "   74.00221     77.002304    80.00239     83.00248     86.00256\n",
            "   89.002655    92.00274     95.00283     98.00292    101.003006\n",
            "  104.0031     107.00318    110.00327    113.00336    116.00345\n",
            "  119.00353    122.003624   125.003716   128.00381    131.00389\n",
            "  134.00398    137.00407    140.00417    143.00426    146.00433\n",
            "  149.00443    152.00452    155.00461    158.00468    161.00478\n",
            "  164.00487    167.00496    170.00505    173.00513    176.00522\n",
            "  179.00531    182.0054     185.00548    188.00557    191.00566\n",
            "  194.00575    197.00584    200.00592    203.00601    206.0061\n",
            "  209.0062     212.00627    215.00636    218.00645    221.00655\n",
            "  224.00664    227.00671    230.0068     233.0069     236.00699\n",
            "  239.00706    242.00716    245.00725    248.00734    251.00743\n",
            "  254.0075     257.0076     260.00766    263.00775    266.00784\n",
            "  269.00793    272.00803    275.00812    278.0082     281.0083\n",
            "  284.0084     287.00848    290.00854    293.00864    296.00873\n",
            "  299.00882  ]\n"
          ]
        }
      ]
    }
  ]
}